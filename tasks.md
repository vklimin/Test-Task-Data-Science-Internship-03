# Тестовые задания

* [Задание 1](#задание-1)
* [Задание 2](#задание-2)

## CoreML

### Задание 1

Мы обучили модель классификации и получили ROC-AUC-метрику со значением 0,2 (метрика посчитана на тестовом). Что это значит?

Выберите один вариант из списка:

* ❌ Модель переобучилась
* ✅ Модель обучалась на неверных данных таргета
* ❌ Модель обучилась хорошо
* ❌ Модель обучилась на несбалансированных данных

---

**Объяснение:**

ROC-AUC = 0,2 — это очень низкое значение (ниже случайного угадывания, которое соответствует 0,5). Это означает, что модель работает хуже шанса. Чаще всего такое происходит, если метки классов (таргет) были перепутаны при обучении — например, все «1» приняты за «0» и наоборот. В этом случае модель может быть по сути "инвертированной", и её предсказания можно легко исправить, просто поменяв знак вероятностей.

---

### Задание 2

Допустим, мы обучили модель бинарной классификации с precision = 80% на выборке из 1000 примеров и предсказали позитивный класс в 150 случаях. Какая полнота у нашей модели, если всего позитивных примеров  в выборке — 240?

Выберите один вариант из списка:

* ❌ 75%
* ❌ 24%
* ✅ 50%
* ❌ 62,5%

---

**Объяснение:**

#### Определения метрик

- **Precision (Точность)** — доля действительно положительных объектов среди тех, что модель пометила как положительные:
  \[
  \text{Precision} = \frac{TP}{TP + FP}
  \]
  где  
  - \( TP \) — истинно положительные (модель сказала "да", и правда "да")  
  - \( FP \) — ложно положительные (модель сказала "да", но на самом деле "нет")

- **Recall (Полнота)** — доля правильно предсказанных положительных примеров среди всех реальных положительных:
  \[
  \text{Recall} = \frac{TP}{TP + FN}
  \]
  где  
  - \( FN \) — ложно отрицательные (модель сказала "нет", но на самом деле "да")

---

#### Поиск количества истинно положительных (TP)

Из условия:
- Precision = 80% = 0.8
- Модель выдала 150 предсказаний класса "1" → это \( TP + FP = 150 \)

Подставим в формулу точности:
\[
\text{Precision} = \frac{TP}{TP + FP} = \frac{TP}{150} = 0.8
\Rightarrow TP = 0.8 \times 150 = 120
\]

Значит, модель **верно** предсказала **120** положительных примеров.

---

#### Поиск полноты (Recall)

Общее число реальных положительных примеров в выборке = 240 → это \( TP + FN = 240 \)

Мы уже знаем, что \( TP = 120 \), значит:
\[
FN = 240 - 120 = 120
\]

Теперь вычислим полноту:
\[
\text{Recall} = \frac{TP}{TP + FN} = \frac{120}{120 + 120} = \frac{120}{240} = 0.5 = 50\%
\]

---
